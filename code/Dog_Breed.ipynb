{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting dog breed\n",
    "In the booming dog insurance industry, the very first step everyone need is to give a accurate quote. \n",
    "Breed is the first thing we need to give a quote. Can you tell the dog breed simply from a photo? Let's find out!\n",
    "\n",
    "Looking at our dataset, we have 8351 photos from 133 classes of dogs, is that a lot of photos? Yes!\n",
    "Is that enough to train a deep learning model that can determine the breed of the dog effectively？  \n",
    "Sadly, No.  \n",
    "Training an effective deep learning models usually take millions of photos running on multiple GPUs and take weeks.  \n",
    "But let's do it anyway!!  \n",
    "Let's build a model from scratch and see how well it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "Building a convolutional neural network is easy, but build a good one that has good performance is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's load file names and targets from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 1.220766544342041\n",
      "8351 images was detected in 133 classes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    # One-hot-encoding the targets\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), len(set(data.target)))\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "t = time.time()\n",
    "data_files, data_targets = load_dataset('../complete/')\n",
    "num_class = data_targets.shape[1]\n",
    "\n",
    "print(\"Time used:\", time.time()-t)\n",
    "print(\"%i images was detected in %s classes.\" %(len(data_files),num_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have altogether 8351 photos from 133 classes of dogs. There are on average 63 photos for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle and Split data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the filenames for train and test\n",
    "F_train, F_test, y_train, y_test = train_test_split(data_files, data_targets, \n",
    "                                                    test_size=0.15, shuffle=True, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have split the data into train and test groups. We use 15% percent data as test group.   \n",
    "What need to be noticed is we only split the file names. We still need too load file names into images before we train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's build our own CNN and see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Dense, Flatten, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "def my_cnn(IMG_SIZE):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3), padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))    \n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(num_class, activation = 'softmax'))\n",
    "    return model\n",
    "\n",
    "my_model = my_cnn(224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start with a relatively simple structure. 5 blocks of convolution with max pooling. A Global average pooling layer after them to reduce overfitting. Batch Normalization is used after every block to prevent overfitting and also improve the result. I only add a Dropout layer after the last batch normalization layer. Dropout layers don't usually go well with batch normalization, but you can use it after the deepest one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see what the model looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 110, 110, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 55, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 55, 55, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 53, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 96)        55392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 10, 10, 32)        27680     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 5, 5, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               34181     \n",
      "=================================================================\n",
      "Total params: 183,173\n",
      "Trainable params: 182,597\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the model relatively simple. With Global Average Pooling, we can keep the number of parameters low. We can see from the model, we only have 182k parameters to tune. Since we have a small data set, this reduce the chance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "Now we can load filenames into images.  \n",
    "Since there are only around 8000 images, we can just load them into memory. As our own model doesn't require a specific image size, we can just go with anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7098/7098 [00:48<00:00, 146.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1253/1253 [00:08<00:00, 147.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "\n",
    "def path_to_tensor(img_path, img_size):\n",
    "    img = image.load_img(img_path, target_size=(img_size, img_size))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths, img_size):\n",
    "    list_of_tensors = [path_to_tensor(img_path, img_size) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "X_train = paths_to_tensor(F_train, 224).astype('float32')/255\n",
    "X_test = paths_to_tensor(F_test, 224).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6033 samples, validate on 1065 samples\n",
      "Epoch 1/12\n",
      " - 20s - loss: 4.6420 - acc: 0.0380 - val_loss: 5.5107 - val_acc: 0.0338\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.51069, saving model to ../models/best_CNN.hdf5\n",
      "Epoch 2/12\n",
      " - 18s - loss: 4.1542 - acc: 0.0744 - val_loss: 4.2626 - val_acc: 0.0601\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.51069 to 4.26264, saving model to ../models/best_CNN.hdf5\n",
      "Epoch 3/12\n",
      " - 17s - loss: 3.8413 - acc: 0.1062 - val_loss: 4.1141 - val_acc: 0.0667\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.26264 to 4.11414, saving model to ../models/best_CNN.hdf5\n",
      "Epoch 4/12\n",
      " - 18s - loss: 3.5954 - acc: 0.1358 - val_loss: 4.3513 - val_acc: 0.0939\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.11414\n",
      "Epoch 5/12\n",
      " - 18s - loss: 3.4038 - acc: 0.1672 - val_loss: 4.6115 - val_acc: 0.0629\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.11414\n",
      "Epoch 6/12\n",
      " - 18s - loss: 3.2304 - acc: 0.1898 - val_loss: 3.8461 - val_acc: 0.1239\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.11414 to 3.84614, saving model to ../models/best_CNN.hdf5\n",
      "Epoch 7/12\n",
      " - 17s - loss: 3.0417 - acc: 0.2148 - val_loss: 4.3935 - val_acc: 0.0779\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3.84614\n",
      "Epoch 8/12\n",
      " - 17s - loss: 2.8923 - acc: 0.2458 - val_loss: 3.6140 - val_acc: 0.1362\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.84614 to 3.61405, saving model to ../models/best_CNN.hdf5\n",
      "Epoch 9/12\n",
      " - 19s - loss: 2.7493 - acc: 0.2717 - val_loss: 3.2432 - val_acc: 0.2150\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.61405 to 3.24324, saving model to ../models/best_CNN.hdf5\n",
      "Epoch 10/12\n",
      " - 18s - loss: 2.6044 - acc: 0.3035 - val_loss: 2.8950 - val_acc: 0.2667\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.24324 to 2.89498, saving model to ../models/best_CNN.hdf5\n",
      "Epoch 11/12\n",
      " - 18s - loss: 2.4400 - acc: 0.3308 - val_loss: 3.7790 - val_acc: 0.1549\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.89498\n",
      "Epoch 12/12\n",
      " - 19s - loss: 2.3424 - acc: 0.3521 - val_loss: 4.5425 - val_acc: 0.1512\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.89498\n",
      "Training takes 219 seconds.\n",
      "[INFO] loss=2.9422, accuracy: 25.4589%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.optimizers import SGD\n",
    "\n",
    "np.random.seed(2018)\n",
    "EPOCH = 12\n",
    "\n",
    "my_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='../models/best_CNN.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "t=time.time()\n",
    "my_model.fit(X_train, y_train, validation_split=0.15, epochs=EPOCH, batch_size=16, \n",
    "             callbacks=[checkpointer], verbose=2)\n",
    "print('Training takes %i seconds.' % (time.time() - t))\n",
    "\n",
    "my_model.load_weights('../models/best_CNN.hdf5')\n",
    "(loss, accuracy) = my_model.evaluate(X_test, y_test, batch_size=10, verbose=2)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on a GPU doesn't take much time.  \n",
    "The result is actually not bad. I thinks it's way better than what I could do. But definitely not enough for a real-world application.  \n",
    "I'm sure we can improve it more by more tuning on the model. But it won't get to the point where it's really useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we don't have a lot of data to train our model effectively, but we still want the job done, we can take advantage of the existing models and weights others have already trained. VGG19, Inception, Xception, ResNet are all very good models and people have trained huge datasets on them.  \n",
    "  \n",
    "  \n",
    "There are several approaches when working with pre-trained models depending on the size of data and similarity between your data and the data that trained the model.\n",
    "\n",
    "1. New dataset is small and similar to original dataset. Since the data is small, it is not a good idea to fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes.\n",
    "\n",
    "2. New dataset is large and similar to the original dataset. Since we have more data, we can have more confidence that we won’t overfit if we were to try to fine-tune through the full network.\n",
    "\n",
    "3. New dataset is small but very different from the original dataset. Since the data is small, it is likely best to only train a linear classifier. Since the dataset is very different, it might not be best to train the classifier form the top of the network, which contains more dataset-specific features. Instead, it might work better to train the SVM classifier from activations somewhere earlier in the network.\n",
    "\n",
    "4. New dataset is large and very different from the original dataset. Since the dataset is very large, we may expect that we can afford to train a ConvNet from scratch. However, in practice it is very often still beneficial to initialize with weights from a pretrained model. In this case, we would have enough data and confidence to fine-tune through the entire network.\n",
    "\n",
    "In our case, our images are similar to the \"imagenet\", and our data set is relatively small. The best way is to use method 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CNN as feature extractor\n",
    "When extracting features, we can use multiple trained networks as feature extractors, and then combine the features to train a classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7098/7098 [00:48<00:00, 145.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1253/1253 [00:08<00:00, 148.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7098/7098 [00:49<00:00, 143.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1253/1253 [00:08<00:00, 142.93it/s]\n",
      "C:\\Users\\zouxi\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7098/7098 [00:48<00:00, 147.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1253/1253 [00:08<00:00, 150.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications import xception\n",
    "from keras.applications import resnet50\n",
    "\n",
    "def feature_extractor(MODEL, img_size, pre_process=None, save_name=None):\n",
    "    input_tensor = Input((img_size, img_size, 3))\n",
    "    base_model = MODEL(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(base_model.input, x)\n",
    "    \n",
    "    X_train = paths_to_tensor(F_train, img_size).astype('float32')\n",
    "    X_test = paths_to_tensor(F_test, img_size).astype('float32')\n",
    "    \n",
    "    if pre_process:\n",
    "        X_train = pre_process(X_train)\n",
    "        X_test = pre_process(X_test)\n",
    "    \n",
    "    Fea_train = model.predict(X_train, batch_size=32)\n",
    "    Fea_test= model.predict(X_test, batch_size=32)\n",
    "    with h5py.File('../models/feature_%s.h5'%save_name, 'w') as h:\n",
    "        h.create_dataset(\"train\", data=Fea_train)\n",
    "        h.create_dataset(\"test\", data=Fea_test)\n",
    "    \n",
    "feature_extractor(inception_v3.InceptionV3, 299, inception_v3.preprocess_input, 'inception')\n",
    "feature_extractor(xception.Xception, 299, xception.preprocess_input, 'xception')\n",
    "feature_extractor(resnet50.ResNet50, 244, resnet50.preprocess_input, 'resnet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the features into different files, so we don't need to extract them every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the feature extracted, we can load them and build a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Fea_train = []\n",
    "Fea_test = []\n",
    "\n",
    "files = ['feature_inception.h5','feature_resnet50.h5','feature_xception.h5']\n",
    "filenames = [os.path.join('../models/', file) for file in files]\n",
    "for filename in filenames:\n",
    "    with h5py.File(filename,'r') as h:\n",
    "        Fea_train.append(np.array(h['train']))\n",
    "        Fea_test.append(np.array(h['test']))\n",
    "Fea_train = np.concatenate(Fea_train, axis=1)\n",
    "Fea_test = np.concatenate(Fea_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6033 samples, validate on 1065 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.0223 - acc: 0.9941 - val_loss: 0.0112 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01115, saving model to ../models/best_feature.hdf5\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.0086 - acc: 0.9974 - val_loss: 0.0098 - val_acc: 0.9973\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01115 to 0.00979, saving model to ../models/best_feature.hdf5\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.0069 - acc: 0.9980 - val_loss: 0.0095 - val_acc: 0.9975\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00979 to 0.00952, saving model to ../models/best_feature.hdf5\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0085 - val_acc: 0.9978\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00952 to 0.00854, saving model to ../models/best_feature.hdf5\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0086 - val_acc: 0.9977\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00854\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0083 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00854 to 0.00830, saving model to ../models/best_feature.hdf5\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0084 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00830\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.0045 - acc: 0.9988 - val_loss: 0.0088 - val_acc: 0.9979\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00830\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0084 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00830\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.0040 - acc: 0.9990 - val_loss: 0.0087 - val_acc: 0.9980\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00830\n"
     ]
    }
   ],
   "source": [
    "def second_model():\n",
    "    input_tensor = Input(Fea_train.shape[1:])\n",
    "    x = Dropout(0.5)(input_tensor)\n",
    "    x = Dense(num_class, activation='sigmoid')(x)\n",
    "    model =Model(input_tensor, x)  \n",
    "    return model\n",
    "\n",
    "second_model = second_model()\n",
    "second_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='../models/best_feature.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "second_model.fit(Fea_train, y_train, batch_size=12, epochs=10, validation_split=0.15,callbacks=[checkpointer], verbose=2)\n",
    "second_model.load_weights('../models/best_feature.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions on test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = second_model.predict(Fea_test)\n",
    "pred = [np.argmax(prediction) for prediction in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90.6624%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = [np.argmax(prediction) for prediction in y_pred]\n",
    "true = [np.argmax(true_label) for true_label in y_test]\n",
    "\n",
    "print('Test accuracy: %.4f%%' % (accuracy_score(true, pred) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve the result, there are a few tricks we can use.\n",
    "1. Expand the data set\n",
    "We can find more data set with dog breed tags and combine them to have more data to train.\n",
    "2. Data augmentation\n",
    "Data augmentation is very commonly used in CNN problems, the basic idea is to modify the original data, such as rotate, shift, flip or expand. This expand the data set itself without external data.  \n",
    "\n",
    "But for our case, I don't think it's required. As a customer, uploading a photo is not necessarily easier then choosing the breed from a drop off list. And we don't have a effective model predicting mixed breed dogs, which is a lot more difficult to identify.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we should focus more on how to make an accurate quote base on the information they provide.   \n",
    "I will share my thought on that in another document \"Dog insurance quote estimation.docx\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
